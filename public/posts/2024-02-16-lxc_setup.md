---
layout: post
title: LXC Cluster w/ Microceph Deployment Guide
date: 2024-02-16
summary: Covers deploying and setting up lxd cluster across multiple nodes integrated with HA microceph storage
categories: guides
image: containerLogo.png
---

## Guide results

A LXD Cluster across atleast 3 nodes utilizing microceph storage.

## Prerequisites

- Everything is done on Ubuntu 22.04 LTS server.
- You must have atleast 2 nodes.

## Begin

Let's jump into the LXC Cluster setup. To start, we will initiate lxd on our `manager` node.

```bash
lxd init
```

You will now be presented with a LOT of questions. Here are the explanations, if any of the anwsers after `:` are blank, it means I just pressed enter to use the default.

```bash
# Create a cluster
Would you like to use LXD clustering? (yes/no) [default=no]: yes
# 'Control-plane' IP for the cluster. Defaults to IP of machine you run it on
What IP address or DNS name should be used to reach this server? [default=192.168.3.200]:
# If you are just initiating it, no. If you are a `worker` joining then yes.
Are you joining an existing cluster? (yes/no) [default=no]: no
# 'Control-plane' Hostname for the cluster. Defaults to Hostname of machine you run it on
What member name should be used to identify this server in the cluster? [default=k8-master-1]:
# Say no implement it with CEPH or other storages.
Do you want to configure a new local storage pool? (yes/no) [default=yes]: no
# We will now add ceph
Do you want to configure a new remote storage pool? (yes/no) [default=no]: yes
# Choose ceph as type
Name of the storage backend to use (ceph, cephfs, cephobject) [default=ceph]: ceph
Create a new CEPH pool? (yes/no) [default=yes]: yes
Name of the OSD storage pool [default=lxd]:
Number of placement groups [default=32]:
# No
Would you like to connect to a MAAS server? (yes/no) [default=no]:
# To get public ip addresses on the containers in the cluster itself then you must anwser yes.
Would you like to configure LXD to use an existing bridge or host interface? (yes/no) [default=no]: yes
# Name of interface for host. Get with -> ip a
Name of the existing bridge or host interface: ens18
# uh
Would you like stale cached images to be updated automatically? (yes/no) [default=yes]:
# No need
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:
```

Before moving on, here is another example using local storage instead of ceph.

```bash
# Create a cluster
Would you like to use LXD clustering? (yes/no) [default=no]: yes
# 'Control-plane' IP for the cluster. Defaults to IP of machine you run it on
What IP address or DNS name should be used to reach this server? [default=192.168.3.200]:
# If you are just initiating it, no. If you are a `worker` joining then yes.
Are you joining an existing cluster? (yes/no) [default=no]: no
# 'Control-plane' Hostname for the cluster. Defaults to Hostname of machine you run it on
What member name should be used to identify this server in the cluster? [default=k8-master-1]:
# We will create a new one. However you can also implement it with CEPH or other storages.
Do you want to configure a new local storage pool? (yes/no) [default=yes]:
# ZFS by default, just type of storage to use
Name of the storage backend to use (btrfs, dir, lvm, zfs) [default=zfs]:
# If you had exisitng pool you could use it, we will create new
Create a new ZFS pool? (yes/no) [default=yes]:
# Do you have a disk you can attach to this? I do not, so i said no.
Would you like to use an existing empty block device (e.g. a disk or partition)? (yes/no) [default=no]: no
# Size
Size in GiB of the new loop device (1GiB minimum) [default=9GiB]: 2GiB
# Again, could be ceph or others, for us no.
Do you want to configure a new remote storage pool? (yes/no) [default=no]: no
# No
Would you like to connect to a MAAS server? (yes/no) [default=no]:
# To get public ip addresses on the containers in the cluster itself then you must anwser yes.
Would you like to configure LXD to use an existing bridge or host interface? (yes/no) [default=no]: yes
# Name of interface for host. Get with -> ip a
Name of the existing bridge or host interface: ens18
# uh
Would you like stale cached images to be updated automatically? (yes/no) [default=yes]:
# No need
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:
```

Okay, now lets join a worker. First, on the `manager` node we just set up, run the following command replacing hostname with the workers hostname you want to join to the cluster.

```bash
lxc cluster add <workerHostname>
```

```bash
sudo lxd init
# Create a cluster
Would you like to use LXD clustering? (yes/no) [default=no]: yes
# 'Control-plane' IP for the cluster. Defaults to IP of machine you run it on
What IP address or DNS name should be used to reach this server? [default=192.168.3.200]:
# If you are a `worker` joining then yes.
Are you joining an existing cluster? (yes/no) [default=no]: yes
# Put in the token we just generated on the `manager` node
Do you have a join token? (yes/no/[token]) [default=no]: <putTokenhere>
# Yes.
All existing data is lost when joining a cluster, continue? (yes/no) [default=no]: yes
# No need
Would you like a YAML "lxd init" preseed to be printed? (yes/no) [default=no]:
```

You can do the above with as many `workers` as you need. Give it a check with

```bash
lxc cluster list
```

Now, lets create an `Ubuntu 22.04` container, then migrate it to one of the workers.

```bash
# You can list your available storage with
lxc storage list

# Use this if you setup local storage
lxc launch images:ubuntu/22.04 <containerName> --storage local
# Use this if you setup remote storage (Ceph)
lxc launch images:ubuntu/22.04 <containerName> --storage remote

# after this runs, we can look at the container with
lxc list

# We can enter the container with
lxc exec <containerName> bash

# Lets migrate it to another host

# First stop it (You can do live migrations with a little bit of setup)
lxc stop <containerName>
lxc move <containerName> --target <targetHostname>
lxc start <containerName>
```
